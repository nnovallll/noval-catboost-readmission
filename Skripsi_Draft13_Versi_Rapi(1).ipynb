{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Library dan Data"
      ],
      "metadata": {
        "id": "hqEGZs8-1bHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Import Library"
      ],
      "metadata": {
        "id": "0TmyKyy61fgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "p4K4v-Af1hes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "2-4EhILt1kRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwQbR37K0-hC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from catboost import CatBoostClassifier, Pool, cv\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_auc_score, average_precision_score,\n",
        "    matthews_corrcoef, balanced_accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score as accuracy_score_sklearn\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "SKAukmWB1oXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "metadata": {
        "id": "pLOjCQ4O1qGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    ConfusionMatrixDisplay\n",
        ")"
      ],
      "metadata": {
        "id": "oecRJ67w1ra2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Import Data"
      ],
      "metadata": {
        "id": "e5_5ib8m1snM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes=pd.read_csv('/content/drive/MyDrive/Skripsi/Dataset/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv')"
      ],
      "metadata": {
        "id": "uYsyOYLS1uYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pre-Processing Data"
      ],
      "metadata": {
        "id": "2exvkkB31yUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Dropping Feature"
      ],
      "metadata": {
        "id": "uynmb5TL116L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop=['race', 'weight', 'medical_specialty', 'max_glu_serum']"
      ],
      "metadata": {
        "id": "T9vhUryd16OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Lookup Table untuk Kode Kategorikal"
      ],
      "metadata": {
        "id": "-9_fDO4h18kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "admission_type_id = { 1 : 'Emergency'\n",
        ", 2 : 'Urgent'\n",
        ", 3 : 'Elective'\n",
        ", 4 : 'Newborn'\n",
        ", 5 : 'Not Available'\n",
        ", 6 : 'NULL'\n",
        ", 7 : 'Trauma Center'\n",
        ", 8 : 'Not Mapped' }"
      ],
      "metadata": {
        "id": "GKcrn9rv2aJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "discharge_disposition_id = { 1 : 'Discharged to home'\n",
        ", 2 : 'Discharged/transferred to another short term hospital'\n",
        ", 3 : 'Discharged/transferred to SNF'\n",
        ", 4 : 'Discharged/transferred to ICF'\n",
        ", 5 : 'Discharged/transferred to another type of inpatient care institution'\n",
        ", 6 : 'Discharged/transferred to home with home health service'\n",
        ", 7 : 'Left AMA'\n",
        ", 8 : 'Discharged/transferred to home under care of Home IV provider'\n",
        ", 9 : 'Admitted as an inpatient to this hospital'\n",
        ", 10 : 'Neonate discharged to another hospital for neonatal aftercare'\n",
        ", 11 : 'Expired'\n",
        ", 12 : 'Still patient or expected to return for outpatient services'\n",
        ", 13 : 'Hospice / home'\n",
        ", 14 : 'Hospice / medical facility'\n",
        ", 15 : 'Discharged/transferred within this institution to Medicare approved swing bed'\n",
        ", 16 : 'Discharged/transferred/referred another institution for outpatient services'\n",
        ", 17 : 'Discharged/transferred/referred to this institution for outpatient services'\n",
        ", 18 : 'NULL'\n",
        ", 19 : 'Expired at home. Medicaid only, hospice'\n",
        ", 20 : 'Expired in a medical facility. Medicaid only, hospice'\n",
        ", 21 : 'Expired, place unknown. Medicaid only, hospice'\n",
        ", 22 : 'Discharged/transferred to another rehab fac including rehab units of a hospital'\n",
        ", 23 : 'Discharged/transferred to a long term care hospital'\n",
        ", 24 : 'Discharged/transferred to a nursing facility certified under Medicaid but not certified under Medicare'\n",
        ", 25 : 'Not Mapped'\n",
        ", 26 : 'Unknown/Invalid'\n",
        ", 30 : 'Discharged/transferred to another Type of Health Care Institution not Defined Elsewhere'\n",
        ", 27 : 'Discharged/transferred to a federal health care facility'\n",
        ", 28 : 'Discharged/transferred/referred to a psychiatric hospital of psychiatric distinct part unit of a hospital'\n",
        ", 29 : 'Discharged/transferred to a Critical Access Hospital (CAH)' }"
      ],
      "metadata": {
        "id": "9ctnCEBp2bhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "admission_source_id = { 1 : 'Physician Referral'\n",
        ", 2 : 'Clinic Referral'\n",
        ", 3 : 'HMO Referral'\n",
        ", 4 : 'Transfer from a hospital'\n",
        ", 5 : 'Transfer from a Skilled Nursing Facility (SNF)'\n",
        ", 6 : 'Transfer from another health care facility'\n",
        ", 7 : 'Emergency Room'\n",
        ", 8 : 'Court/Law Enforcement'\n",
        ", 9 :  'Not Available'\n",
        ", 10 : 'Transfer from critial access hospital'\n",
        ", 11 : 'Normal Delivery'\n",
        ", 12 : 'Premature Delivery'\n",
        ", 13 : 'Sick Baby'\n",
        ", 14 : 'Extramural Birth'\n",
        ", 15 : 'Not Available'\n",
        ", 17 : 'NULL'\n",
        ", 18 : 'Transfer From Another Home Health Agency'\n",
        ", 19 : 'Readmission to Same Home Health Agency'\n",
        ", 20 : 'Not Mapped'\n",
        ", 21 : 'Unknown/Invalid'\n",
        ", 22 : 'Transfer from hospital inpt/same fac reslt in a sep claim'\n",
        ", 23 : 'Born inside this hospital'\n",
        ", 24 : 'Born outside this hospital'\n",
        ", 25 : 'Transfer from Ambulatory Surgery Center'\n",
        ", 26 : 'Transfer from Hospice'\n",
        "                      }"
      ],
      "metadata": {
        "id": "CQyuOWnP2dQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "death_disposition_id = {  11 : 'Expired'\n",
        ", 13 : 'Hospice / home'\n",
        ", 14 : 'Hospice / medical facility'\n",
        ", 19 : 'Expired at home. Medicaid only, hospice'\n",
        ", 20 : 'Expired in a medical facility. Medicaid only, hospice'\n",
        ", 21 : 'Expired, place unknown. Medicaid only, hospice'\n",
        " }"
      ],
      "metadata": {
        "id": "QCFWTRXh2fa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes['expiration_ind'] = diabetes['discharge_disposition_id'].isin([11,13,14,19,20,21]).astype('int')"
      ],
      "metadata": {
        "id": "Teo6txV92oOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Filtering Data"
      ],
      "metadata": {
        "id": "81cjJZpw2sWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete patients whose history includes childhood or old age\n",
        "diabetes['under10_ind'] = diabetes['age'].isin(['[0-10)']).astype(int)\n",
        "diabetes['over80_ind']  = diabetes['age'].isin(['[80-90)', '[90-100)']).astype(int)\n",
        "\n",
        "diabetes['mb_under10_ind'] = diabetes.groupby('patient_nbr')['under10_ind'].transform('max')\n",
        "diabetes['mb_over80_ind'] = diabetes.groupby('patient_nbr')['over80_ind'].transform('max')\n"
      ],
      "metadata": {
        "id": "NcYt1mlM2vzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes['expiration_ind'] = diabetes['discharge_disposition_id'].isin([11,13,14,19,20,21]).astype('int')\n",
        "diabetes['encounter_ct'] = diabetes['patient_nbr'].map(diabetes.groupby('patient_nbr')['encounter_id'].nunique())"
      ],
      "metadata": {
        "id": "Avt57C2W2yDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(diabetes.shape)\n",
        "#remove very young and very old members, members who died, unknown gender\n",
        "diabetes = diabetes[\n",
        "    (diabetes['expiration_ind'] != 1 ) &\n",
        "    (diabetes['mb_under10_ind'] != 1 ) &\n",
        "    (diabetes['mb_over80_ind']  != 1 ) &\n",
        "    (diabetes['gender'] != 'Unknown/Invalid') &  # Exclude records with gender 'Unknown/Invalid'\n",
        "    (diabetes['encounter_ct'] < 14 )  # Exclude records with gender 'Unknown/Invalid'\n",
        "]\n",
        "print(diabetes.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "QzeX_9nE2zRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes=diabetes.drop(['mb_under10_ind', 'mb_over80_ind', 'expiration_ind','under10_ind', 'over80_ind'], axis =1)\n"
      ],
      "metadata": {
        "id": "13XUirME21Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Membuat Variabel Target"
      ],
      "metadata": {
        "id": "SwdWNpqe244p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes['readmitted_lt30_ind'] = ( diabetes['readmitted']=='<30' ).astype(int)\n",
        "diabetes['readmitted_gt30_ind'] = ( diabetes['readmitted']=='>30' ).astype(int)\n",
        "diabetes['readmitted_no_ind'] = ( diabetes['readmitted']=='NO' ).astype(int)\n",
        "diabetes['readmitted_ind'] = diabetes['readmitted_lt30_ind'] + diabetes['readmitted_gt30_ind']\n",
        "\n"
      ],
      "metadata": {
        "id": "rAffzzv0220s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes = diabetes.drop(['readmitted_no_ind', 'readmitted_gt30_ind','readmitted_lt30_ind'], axis=1)"
      ],
      "metadata": {
        "id": "GEWlBGGX2_Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Split Train Test Data"
      ],
      "metadata": {
        "id": "Xp50wbiE3EDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sigO1IPO3GvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean=diabetes.copy()"
      ],
      "metadata": {
        "id": "2FPflOBf3OcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 1: Buat kelas representatif untuk setiap pasien\n",
        "# Logika: Jika pasien pernah readmitted (nilai max = 1), maka kelasnya 1.\n",
        "print(\"1. Membuat ringkasan kelas per pasien...\")\n",
        "patient_summary = df_clean.groupby('patient_nbr')['readmitted_ind'].max().reset_index()\n",
        "patient_summary.rename(columns={'readmitted_ind': 'patient_class'}, inplace=True)"
      ],
      "metadata": {
        "id": "x3KAkupL3QMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 2: Lakukan stratified split pada DAFTAR PASIEN (bukan seluruh data)\n",
        "print(\"2. Melakukan pemisahan ID pasien secara terstratifikasi...\")\n",
        "train_ids, test_ids = train_test_split(\n",
        "    patient_summary['patient_nbr'],\n",
        "    # patient_summary['patient_class'], # We are splitting patient_nbr, not two separate arrays\n",
        "    test_size=0.10, # Alokasi 20% pasien untuk data uji\n",
        "    random_state=42,\n",
        "    stratify=patient_summary['patient_class']\n",
        ")"
      ],
      "metadata": {
        "id": "JTYaBGeM3Tlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 3: Buat DataFrame train dan test final berdasarkan ID pasien\n",
        "print(\"3. Membuat DataFrame final...\")\n",
        "train_df = df_clean[df_clean['patient_nbr'].isin(train_ids)].copy()\n",
        "test_df = df_clean[df_clean['patient_nbr'].isin(test_ids)].copy()"
      ],
      "metadata": {
        "id": "JqasYGzH3Xtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VERIFIKASI HASIL ---\n",
        "print(\"\\n--- Hasil Pemisahan Data ---\")\n",
        "print(f\"Jumlah baris data latih: {len(train_df)}\")\n",
        "print(f\"Jumlah baris data uji: {len(test_df)}\")\n",
        "print(\"-\" * 30)\n",
        "print(\"Distribusi target di Data Latih:\")\n",
        "print(train_df['readmitted_ind'].value_counts(normalize=True))\n",
        "print(\"\\nDistribusi target di Data Uji:\")\n",
        "print(test_df['readmitted_ind'].value_counts(normalize=True))\n",
        "print(\"-\" * 30)\n",
        "# Cek apakah ada irisan pasien\n",
        "common_patients = set(train_df['patient_nbr']) & set(test_df['patient_nbr'])\n",
        "print(f\"Jumlah pasien yang beririsan: {len(common_patients)}\")"
      ],
      "metadata": {
        "id": "nw2Z4_cn3ZBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan Anda sudah menjalankan kode untuk membuat train_df dan test_df terlebih dahulu\n",
        "\n",
        "# 1. Ambil daftar patient_nbr unik dari masing-masing set\n",
        "patient_nbr_train = set(train_df['patient_nbr'])\n",
        "patient_nbr_test = set(test_df['patient_nbr'])\n",
        "\n",
        "# 2. Cari irisan (intersection) antara dua set tersebut\n",
        "common_patient_nbr = patient_nbr_train.intersection(patient_nbr_test)\n",
        "\n",
        "# 3. Tampilkan hasilnya\n",
        "if common_patient_nbr:\n",
        "    print(f\"❌ DITEMUKAN KEBOCORAN: Ada {len(common_patient_nbr)} ID pasien yang sama di data latih dan uji.\")\n",
        "    # print(\"Contoh ID yang beririsan:\", list(common_patient_nbr)[:5]) # Uncomment untuk melihat contoh\n",
        "else:\n",
        "    print(\"✅ TIDAK ADA KEBOCORAN: Tidak ada satupun ID pasien yang sama antara data latih dan uji.\")"
      ],
      "metadata": {
        "id": "I71-nekO3bM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering"
      ],
      "metadata": {
        "id": "TjrD4mfh3f3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Install Values from lookup dictionaries"
      ],
      "metadata": {
        "id": "5FjTCYXa3nnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_admission_discharge_features(df, admission_type_id, discharge_disposition_id, admission_source_id, drop):\n",
        "    \"\"\"\n",
        "    Menambahkan fitur admission/discharge ke dataframe dan mengupdate list drop.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame input dengan kolom id terkait admission/discharge.\n",
        "        admission_type_id (dict): Mapping admission_type_id ke kategori.\n",
        "        discharge_disposition_id (dict): Mapping discharge_disposition_id ke kategori.\n",
        "        admission_source_id (dict): Mapping admission_source_id ke kategori.\n",
        "        drop (list): List kolom yang akan dihapus, akan diupdate dalam fungsi ini.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan fitur tambahan.\n",
        "        list: List drop yang telah diperbarui.\n",
        "    \"\"\"\n",
        "\n",
        "    # Mapping ID ke kategori\n",
        "    df['admission_type'] = df['admission_type_id'].map(admission_type_id)\n",
        "    df['discharge_disposition'] = df['discharge_disposition_id'].map(discharge_disposition_id)\n",
        "    df['admission_source'] = df['admission_source_id'].map(admission_source_id)\n",
        "\n",
        "    # Fitur payer_code: MP atau DM\n",
        "    df['MP_DM_payer_ind'] = ((df['payer_code'] == 'MP') | (df['payer_code'] == 'DM')).astype(int)\n",
        "\n",
        "    # Grup admission\n",
        "    df['admission_grp_1_ind'] = df['admission_type'].isin(['NULL','Emergency']).astype(int)\n",
        "    df['admission_grp_2_ind'] = df['admission_type'].isin(['Elective','Not Mapped']).astype(int)\n",
        "\n",
        "    # Grup discharge\n",
        "    df['discharge_grp_1_ind'] = df['discharge_disposition'].isin([\n",
        "        'Discharged/transferred to a long term care hospital',\n",
        "        'NULL',\n",
        "        'Discharged to home'\n",
        "    ]).astype(int)\n",
        "\n",
        "    df['discharge_grp_2_ind'] = df['discharge_disposition'].isin([\n",
        "        'Left AMA',\n",
        "        'Discharged/transferred to another type of inpatient care institution',\n",
        "        'Discharged/transferred to SNF',\n",
        "        'Discharged/transferred to home with home health service',\n",
        "        'Discharged/transferred to another rehab fac including rehab units of a hospital'\n",
        "    ]).astype(int)\n",
        "\n",
        "    # Fitur admission_type_ind\n",
        "    df['admission_type_ind'] = df['admission_source'].isin([\n",
        "        'Clinic Referral',\n",
        "        'Transfer from a hospital',\n",
        "        'Transfer from another health care facility'\n",
        "    ]).astype(int)\n",
        "\n",
        "    # Agregasi berdasarkan patient_nbr\n",
        "    df['mb_admission_grp_1_ct'] = df.groupby('patient_nbr')['admission_grp_1_ind'].transform('sum')\n",
        "    df['mb_admission_grp_2_ct'] = df.groupby('patient_nbr')['admission_grp_2_ind'].transform('sum')\n",
        "    df['mb_discharge_grp_1_ct'] = df.groupby('patient_nbr')['discharge_grp_1_ind'].transform('sum')\n",
        "    df['mb_discharge_grp_2_ct'] = df.groupby('patient_nbr')['discharge_grp_2_ind'].transform('sum')\n",
        "    df['mb_admission_type_ct']  = df.groupby('patient_nbr')['admission_type_ind'].transform('sum')\n",
        "\n",
        "    # Update list kolom yang akan di-drop\n",
        "    drop.extend([\n",
        "        'payer_code',\n",
        "        'admission_type_id',\n",
        "        'discharge_disposition_id',\n",
        "        'admission_source_id',\n",
        "        'admission_grp_1_ind',\n",
        "        'admission_grp_2_ind',\n",
        "        'discharge_grp_1_ind',\n",
        "        'discharge_grp_2_ind',\n",
        "        'admission_type_ind'\n",
        "    ])\n",
        "\n",
        "    return df, drop\n"
      ],
      "metadata": {
        "id": "2ylRn9cN3ksi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Misal mapping dictionary sudah didefinisikan sebelumnya:\n",
        "# admission_type_id = {...}\n",
        "# discharge_disposition_id = {...}\n",
        "# admission_source_id = {...}\n",
        "\n",
        "drop = []  # atau list drop yang sudah ada sebelumnya\n",
        "\n",
        "train_df, drop = apply_admission_discharge_features(train_df, admission_type_id, discharge_disposition_id, admission_source_id, drop)\n",
        "test_df, drop = apply_admission_discharge_features(test_df, admission_type_id, discharge_disposition_id, admission_source_id, drop)\n"
      ],
      "metadata": {
        "id": "a5kPigdL30Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Standarisasi Kode Diagnosis dan mengganti missing values dengan 'ZZZ'"
      ],
      "metadata": {
        "id": "XKV8lutp3_gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Fungsi untuk Membersihkan Kolom Diagnosis (In-place) ---\n",
        "\n",
        "def clean_diag_codes_inplace(df):\n",
        "    \"\"\"\n",
        "    Membersihkan kolom diag_1, diag_2, dan diag_3 secara in-place.\n",
        "    - Mengambil 3 karakter pertama.\n",
        "    - Mengganti '?' dengan 'ZZZ'.\n",
        "    \"\"\"\n",
        "    diag_columns = ['diag_1', 'diag_2', 'diag_3']\n",
        "\n",
        "    for col in diag_columns:\n",
        "        if col in df.columns:\n",
        "            # Lakukan operasi secara berantai dan langsung ubah kolom di DataFrame asli\n",
        "            df[col] = df[col].astype(str).str[:3].replace('?', 'ZZZ')\n",
        "\n",
        "# --- Cara Penggunaan ---\n",
        "\n",
        "# Asumsikan Anda sudah memiliki train_df, val_df, test_df\n",
        "# Jika belum, silakan jalankan kode split dari jawaban sebelumnya.\n",
        "\n",
        "# Contoh pembuatan data dummy (GANTI DENGAN DATA ANDA)\n",
        "# train_df = pd.DataFrame({\n",
        "#     'diag_1': ['250.00', '401', '?', 'V58.67'],\n",
        "#     'diag_2': ['428', '250.01', '786', '?'],\n",
        "#     'diag_3': ['V10', '?', '414', '250']\n",
        "# })\n",
        "# test_df = train_df.copy() # Hanya untuk contoh\n",
        "\n",
        "# Terapkan fungsi ke setiap DataFrame\n",
        "print(\"Membersihkan data latih...\")\n",
        "clean_diag_codes_inplace(train_df)\n",
        "\n",
        "print(\"Membersihkan data uji...\")\n",
        "clean_diag_codes_inplace(test_df)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Fungsi untuk Membersihkan Kolom Diagnosis (In-place) ---\n",
        "\n",
        "def clean_diag_codes_inplace(df):\n",
        "    \"\"\"\n",
        "    Membersihkan kolom diag_1, diag_2, dan diag_3 secara in-place.\n",
        "    - Mengambil 3 karakter pertama.\n",
        "    - Mengganti '?' dengan 'ZZZ'.\n",
        "    \"\"\"\n",
        "    diag_columns = ['diag_1', 'diag_2', 'diag_3']\n",
        "\n",
        "    for col in diag_columns:\n",
        "        if col in df.columns:\n",
        "            # Lakukan operasi secara berantai dan langsung ubah kolom di DataFrame asli\n",
        "            df[col] = df[col].astype(str).str[:3].replace('?', 'ZZZ')\n",
        "\n",
        "# --- Cara Penggunaan ---\n",
        "\n",
        "# Asumsikan Anda sudah memiliki train_df, val_df, test_df\n",
        "# Jika belum, silakan jalankan kode split dari jawaban sebelumnya.\n",
        "\n",
        "# Contoh pembuatan data dummy (GANTI DENGAN DATA ANDA)\n",
        "# train_df = pd.DataFrame({\n",
        "#     'diag_1': ['250.00', '401', '?', 'V58.67'],\n",
        "#     'diag_2': ['428', '250.01', '786', '?'],\n",
        "#     'diag_3': ['V10', '?', '414', '250']\n",
        "# })\n",
        "# test_df = train_df.copy() # Hanya untuk contoh\n",
        "\n",
        "# Terapkan fungsi ke setiap DataFrame\n",
        "print(\"Membersihkan data latih...\")\n",
        "clean_diag_codes_inplace(train_df)\n",
        "\n",
        "\n",
        "print(\"Membersihkan data uji...\")\n",
        "clean_diag_codes_inplace(test_df)\n",
        "\n",
        "\n",
        "# --- Verifikasi Hasil ---\n",
        "print(\"\\n--- Hasil Pembersihan pada train_df ---\")\n",
        "print(train_df[['diag_1', 'diag_2', 'diag_3']].head())"
      ],
      "metadata": {
        "id": "mIV4kMT34HUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Membuat patient-level count of unique diagnoses codes"
      ],
      "metadata": {
        "id": "_3SAwNvI4J1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_distinct_diag_count(df):\n",
        "    \"\"\"\n",
        "    Menambahkan kolom 'distinct_diag_count' ke dataframe input berdasarkan jumlah diagnosis unik\n",
        "    dari kolom 'diag_1', 'diag_2', dan 'diag_3' untuk setiap 'patient_nbr'.\n",
        "\n",
        "    Parameter:\n",
        "        df (pd.DataFrame): DataFrame yang memiliki kolom 'patient_nbr', 'diag_1', 'diag_2', 'diag_3'\n",
        "\n",
        "    Return:\n",
        "        pd.DataFrame: DataFrame dengan tambahan kolom 'distinct_diag_count'\n",
        "    \"\"\"\n",
        "    diagnosis_melted = df.melt(\n",
        "        id_vars=['patient_nbr'],\n",
        "        value_vars=['diag_1', 'diag_2', 'diag_3'],\n",
        "        var_name='diag_position',\n",
        "        value_name='diagnosis_code'\n",
        "    )\n",
        "\n",
        "    # Hitung jumlah diagnosis unik per pasien\n",
        "    distinct_counts = diagnosis_melted.groupby('patient_nbr')['diagnosis_code'].nunique().reset_index()\n",
        "    distinct_counts.columns = ['patient_nbr', 'distinct_diag_count']\n",
        "\n",
        "    # Gabungkan kembali ke df awal\n",
        "    df = df.merge(distinct_counts, on='patient_nbr', how='left')\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "udX0jpiu4Sup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = add_distinct_diag_count(train_df)\n",
        "test_df = add_distinct_diag_count(test_df)\n"
      ],
      "metadata": {
        "id": "Njw0JTiv4UoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Membuat Dictionary Diag"
      ],
      "metadata": {
        "id": "kHEhrrGs4V4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_diagnosis_codes(df, diag_csv_path):\n",
        "    \"\"\"\n",
        "    Membaca file CSV berisi lookup kode diagnosis, mengubahnya menjadi dictionary, dan memetakan\n",
        "    deskripsi diagnosis ke kolom diagnosis_1, diagnosis_2, diagnosis_3 berdasarkan diag_1-3.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame input yang memiliki kolom 'diag_1', 'diag_2', 'diag_3'.\n",
        "        diag_csv_path (str): Path ke file CSV lookup yang berisi 'diagnosis_cd' dan 'diagnosis'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan kolom tambahan 'diagnosis_1', 'diagnosis_2', 'diagnosis_3'.\n",
        "    \"\"\"\n",
        "    # Baca CSV ke dataframe\n",
        "    lookup_df = pd.read_csv(diag_csv_path)\n",
        "\n",
        "    # Buat dictionary dari lookup\n",
        "    diag_dict = dict(zip(lookup_df['diagnosis_cd'], lookup_df['diagnosis']))\n",
        "\n",
        "    # Tambahkan kode 'ZZZ' sebagai default\n",
        "    diag_dict['ZZZ'] = 'No diag'\n",
        "\n",
        "    # Mapping deskripsi diagnosis\n",
        "    df['diagnosis_1'] = df['diag_1'].map(diag_dict)\n",
        "    df['diagnosis_2'] = df['diag_2'].map(diag_dict)\n",
        "    df['diagnosis_3'] = df['diag_3'].map(diag_dict)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "TLIVshNd4-xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path ke file CSV lookup\n",
        "lookup_path = \"/content/drive/MyDrive/Skripsi/Dataset/diabetes+130-us+hospitals+for+years+1999-2008/unique_diag_df_edit.csv\"\n",
        "\n",
        "# Terapkan fungsi ke dataframe\n",
        "train_df = map_diagnosis_codes(train_df, lookup_path)\n",
        "test_df = map_diagnosis_codes(test_df, lookup_path)\n"
      ],
      "metadata": {
        "id": "8dC47MIl5AdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Membuat Indeks untuk Major Drive kasus readmitted"
      ],
      "metadata": {
        "id": "wHbXA80B5Bib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_major_diag_indicators(df):\n",
        "    \"\"\"\n",
        "    Menambahkan kolom indikator biner untuk diagnosis utama yang sering muncul\n",
        "    pada kolom diag_1, diag_2, dan diag_3.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame input yang memiliki kolom 'diag_1', 'diag_2', dan 'diag_3'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan kolom indikator diagnosis tambahan.\n",
        "    \"\"\"\n",
        "\n",
        "    # diag_1 indicators\n",
        "    df['diag_1_428_ind'] = (df['diag_1'] == '428').astype(int)  # CHF NOS\n",
        "    df['diag_1_491_ind'] = (df['diag_1'] == '491').astype(int)  # SIMPLE CHR BRONCHITIS\n",
        "    df['diag_1_493_ind'] = (df['diag_1'] == '493').astype(int)  # EXTRINSIC ASTHMA NOS\n",
        "\n",
        "    # diag_2 indicators\n",
        "    df['diag_2_403_ind'] = (df['diag_2'] == '403').astype(int)  # MAL HY KID W CR KID I-IV\n",
        "    df['diag_2_707_ind'] = (df['diag_2'] == '707').astype(int)  # DECUBITUS ULCER\n",
        "    df['diag_2_585_ind'] = (df['diag_2'] == '585').astype(int)  # CHRONIC RENAL FAILURE\n",
        "    df['diag_2_491_ind'] = (df['diag_2'] == '491').astype(int)  # SIMPLE CHR BRONCHITIS\n",
        "\n",
        "    # diag_3 indicators\n",
        "    df['diag_3_403_ind'] = (df['diag_3'] == '403').astype(int)  # MAL HY KID W CR KID I-IV\n",
        "    df['diag_3_585_ind'] = (df['diag_3'] == '585').astype(int)  # CHRONIC RENAL FAILURE\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "GJ62Yvze5GjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = add_major_diag_indicators(train_df)\n",
        "test_df = add_major_diag_indicators(test_df)\n"
      ],
      "metadata": {
        "id": "su5LL0cU5Nh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Membuat Indeks untuk kumpulan diagnosis yg mengakibatkan kasus readmiited"
      ],
      "metadata": {
        "id": "T107as8J5O1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_diag_driver_indicators(df):\n",
        "    \"\"\"\n",
        "    Menambahkan indikator biner untuk diagnosis utama (driver) pada diag_1, diag_2, dan diag_3.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame yang memiliki kolom 'diag_1', 'diag_2', dan 'diag_3'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan kolom 'diag_1_driver_ind', 'diag_2_driver_ind', dan 'diag_3_driver_ind'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Daftar kode driver untuk masing-masing kolom\n",
        "    diag_1_drivers = [\n",
        "    'V58', '443', '537', '730', '403', '404', '572', '292', '787', '707',\n",
        "    '398', '280', '295', '996', '285', '440', '536', '188', '733', '789',\n",
        "    '577', '593', '276', '8', '482', '250', '569', '578', '558']\n",
        "\n",
        "    diag_2_drivers = [\n",
        "    '536', '571', '304', '202', '404', '396', '444', '790', '535', '8',\n",
        "    '730', '425', '682', '135', '428', 'V58', '440', '153', '733', '204',\n",
        "    '577', '303', '496', '996', '112', '593']\n",
        "\n",
        "    diag_3_drivers = [\n",
        "    '428', '496', '425', '424', '682', '491', '280', '799', '571', '295',\n",
        "    '789', '535', '70', '996', '536', '578', 'V42', '300', 'E87', '396',\n",
        "    '440', '453', '303', '443', 'V12', '730', '416', '304', '515', '397',\n",
        "    '577', '433', '284', '781', '466', '731', '459','707']\n",
        "\n",
        "    # Buat kolom indikator\n",
        "    df['diag_1_driver_ind'] = df['diag_1'].isin(diag_1_drivers).astype(int)\n",
        "    df['diag_2_driver_ind'] = df['diag_2'].isin(diag_2_drivers).astype(int)\n",
        "    df['diag_3_driver_ind'] = df['diag_3'].isin(diag_3_drivers).astype(int)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "VUEHy5VD5c-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = add_diag_driver_indicators(train_df)\n",
        "test_df = add_diag_driver_indicators(test_df)"
      ],
      "metadata": {
        "id": "clrO76yZ5f4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 Membuat Diagnosis Array dan Tuple"
      ],
      "metadata": {
        "id": "cIaCOsMH5hoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_diagnosis_dictionary(csv_path):\n",
        "    df_lookup = pd.read_csv(csv_path)\n",
        "    diag_dict = dict(zip(df_lookup['diagnosis_cd'], df_lookup['diagnosis']))\n",
        "    diag_dict['ZZZ'] = 'No diag'  # Default value untuk kode diagnosis tak dikenal\n",
        "    return diag_dict\n"
      ],
      "metadata": {
        "id": "WFUQNm-X5rmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diag_dict = load_diagnosis_dictionary(\"/content/drive/MyDrive/Skripsi/Dataset/diabetes+130-us+hospitals+for+years+1999-2008/unique_diag_df_edit.csv\")\n"
      ],
      "metadata": {
        "id": "6hKWjea45tW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_diagnosis_combinations(df, diag_dict):\n",
        "    \"\"\"\n",
        "    Memproses kombinasi diagnosis (diag_1, diag_2, diag_3) untuk menghasilkan diagnosis kombinasi terurut\n",
        "    dan deskripsi diagnosis berdasarkan kamus kode diagnosis.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame yang memiliki kolom 'diag_1', 'diag_2', 'diag_3'.\n",
        "        diag_dict (dict): Dictionary yang memetakan kode diagnosis ke deskripsi diagnosis.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan kolom diagnosis kombinasi dan diagnosis deskriptif.\n",
        "    \"\"\"\n",
        "\n",
        "    # Gabungkan diagnosis ke dalam array lalu urutkan\n",
        "    df['diagnosis_array'] = df[['diag_1', 'diag_2', 'diag_3']].values.tolist()\n",
        "    df['diagnosis_array'] = df['diagnosis_array'].apply(sorted)\n",
        "\n",
        "    # Simpan tuple dan hitung frekuensi\n",
        "    df['diagnosis_array_tuple'] = df['diagnosis_array'].apply(tuple)\n",
        "    df['diagnosis_tuple_freq'] = df.groupby('diagnosis_array_tuple')['diagnosis_array_tuple'].transform('count')\n",
        "\n",
        "    # Kembalikan diagnosis urut ke kolom diag_1_sort, diag_2_sort, diag_3_sort\n",
        "    df[['diag_1_sort', 'diag_2_sort', 'diag_3_sort']] = pd.DataFrame(df['diagnosis_array'].tolist(), index=df.index)\n",
        "\n",
        "    # Hitung frekuensi individual\n",
        "    df['diag_1_sort_freq'] = df.groupby('diag_1_sort')['diag_1_sort'].transform('count')\n",
        "    df['diag_2_sort_freq'] = df.groupby('diag_2_sort')['diag_2_sort'].transform('count')\n",
        "    df['diag_3_sort_freq'] = df.groupby('diag_3_sort')['diag_3_sort'].transform('count')\n",
        "\n",
        "    # Buat string kombinasi\n",
        "    df['diagnosis_tuple'] = df['diagnosis_array'].apply(lambda x: f\"({' '.join(x)})\")\n",
        "\n",
        "    # Drop kolom temporer\n",
        "    df.drop(columns=['diagnosis_array', 'diagnosis_array_tuple'], inplace=True)\n",
        "\n",
        "    # Deskripsi diagnosis yang sudah diurutkan, pakai get biar aman\n",
        "    df['diagnosis_1_sort'] = df['diag_1_sort'].apply(lambda x: diag_dict.get(x, 'Unknown'))\n",
        "    df['diagnosis_2_sort'] = df['diag_2_sort'].apply(lambda x: diag_dict.get(x, 'Unknown'))\n",
        "    df['diagnosis_3_sort'] = df['diag_3_sort'].apply(lambda x: diag_dict.get(x, 'Unknown'))\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "PG6xrYI75u3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load kamus\n",
        "diag_dict = load_diagnosis_dictionary('/content/drive/MyDrive/Skripsi/Dataset/diabetes+130-us+hospitals+for+years+1999-2008/unique_diag_df_edit.csv')\n",
        "\n",
        "# Step 2: Terapkan ke data split-an\n",
        "train_df = process_diagnosis_combinations(train_df, diag_dict)\n",
        "test_df  = process_diagnosis_combinations(test_df, diag_dict)\n"
      ],
      "metadata": {
        "id": "wGAMEVKV5w0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8 Membuat Index yang memiliki High Propensity Tuple"
      ],
      "metadata": {
        "id": "u-6_tWrf5x0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_high_propensity_indicator(df, high_propensity_list):\n",
        "    \"\"\"\n",
        "    Menambahkan kolom indikator untuk menandai apakah diagnosis_tuple termasuk\n",
        "    dalam daftar kombinasi diagnosis dengan propensitas tinggi.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame yang memiliki kolom 'diagnosis_tuple'.\n",
        "        high_propensity_list (list of str): Daftar kombinasi diagnosis yang dianggap memiliki propensitas tinggi.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan kolom tambahan 'high_propensity_tuple_ind'.\n",
        "    \"\"\"\n",
        "    df['high_propensity_tuple_ind'] = df['diagnosis_tuple'].isin(high_propensity_list).astype(int)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "lc7_dqCQ52-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar diagnosis kombinasi dengan propensitas tinggi\n",
        "high_propensity_combinations = ['(250 403 428)'\n",
        ", '(250 428 491)'\n",
        ", '(250 276 428)'\n",
        ", '(250 276 403)'\n",
        ", '(424 427 428)'\n",
        ", '(250 403 996)'\n",
        ", '(250 428 496)'\n",
        ", '(250 425 428)'\n",
        ", '(427 428 491)'\n",
        ", '(250 682 707)'\n",
        ", '(428 491 518)'\n",
        ", '(427 428 496)'\n",
        ", '(425 427 428)'\n",
        ", '(250 403 585)'\n",
        ", '(427 427 428)'\n",
        ", '(428 486 491)'\n",
        ", '(403 428 585)'\n",
        ", '(250 440 707)'\n",
        ", '(250 707 730)'\n",
        ", '(486 491 518)'\n",
        ", '(250 401 428)'\n",
        ", '(427 428 486)'\n",
        ", '(411 414 496)'\n",
        ", '(250 414 428)'\n",
        "]\n",
        "\n",
        "# Terapkan ke DataFrame (misalnya `train_df`)\n",
        "train_df = assign_high_propensity_indicator(train_df, high_propensity_combinations)\n",
        "test_df  = assign_high_propensity_indicator(test_df, high_propensity_combinations)\n"
      ],
      "metadata": {
        "id": "xuPZMLrc55tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.9 Melakukan Selective One Hot Encoding"
      ],
      "metadata": {
        "id": "cmndc2-y57lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dx_indicators(df, dx_list):\n",
        "    \"\"\"\n",
        "    Membuat kolom indikator (binary 0/1) untuk setiap kode diagnosis dalam dx_list.\n",
        "    Kolom indikator menandakan apakah kode diagnosis tersebut muncul di diag_1, diag_2, atau diag_3.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame yang memiliki kolom 'patient_nbr', 'encounter_id', 'diag_1', 'diag_2', 'diag_3'.\n",
        "        dx_list (list of str): Daftar kode diagnosis yang akan dicari dan dibuatkan kolom indikatornya.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame baru dengan kolom 'patient_nbr', 'encounter_id', 'diag_1', 'diag_2', 'diag_3',\n",
        "                      ditambah kolom dx_*_ind untuk setiap kode di dx_list.\n",
        "    \"\"\"\n",
        "    # Salin kolom-kolom dasar\n",
        "    dx_events = df[['patient_nbr', 'encounter_id', 'diag_1', 'diag_2', 'diag_3']].copy(deep=True)\n",
        "\n",
        "    # Buat kolom indikator untuk setiap diagnosis dalam dx_list\n",
        "    for dx in dx_list:\n",
        "        dx_events[f'dx_{dx}_ind'] = ((dx_events['diag_1'] == dx) |\n",
        "                                     (dx_events['diag_2'] == dx) |\n",
        "                                     (dx_events['diag_3'] == dx)).astype(int)\n",
        "\n",
        "    return dx_events\n"
      ],
      "metadata": {
        "id": "k8nvoWDj6ENA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx_list = [ '282'\n",
        ",'536'\n",
        ",'581'\n",
        ",'403'\n",
        ",'585'\n",
        ",'583'\n",
        ",'491'\n",
        ",'537'\n",
        ",'443'\n",
        ",'277'\n",
        ",'730'\n",
        ",'428'\n",
        ",'404']\n"
      ],
      "metadata": {
        "id": "6rGN8RWA6FtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat indikator untuk masing-masing set\n",
        "train_dx = create_dx_indicators(train_df, dx_list)\n",
        "test_dx  = create_dx_indicators(test_df, dx_list)\n"
      ],
      "metadata": {
        "id": "WFHU-GEc6HLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.10 Melakukan Aggregat per Pasien (Berdasarkan pasien number)"
      ],
      "metadata": {
        "id": "mTSYmDRt6I5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_dx_indicators(dx_events: pd.DataFrame, dx_list: list, output_csv: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Menggabungkan data dx_events berdasarkan patient_nbr, lalu menghitung max dan sum\n",
        "    untuk setiap diagnosis indicator dx_list.\n",
        "\n",
        "    Parameters:\n",
        "        dx_events (pd.DataFrame): DataFrame yang memiliki kolom 'patient_nbr', 'encounter_id', 'diag_1', 'diag_2', 'diag_3',\n",
        "                                  serta kolom indikator diagnosis seperti dx_428_ind, dx_585_ind, dst.\n",
        "        dx_list (list): List kode diagnosis yang digunakan sebagai dasar penamaan kolom.\n",
        "        output_csv (str, optional): Nama file untuk menyimpan hasil agregasi ke CSV. Jika None, tidak disimpan.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame hasil agregasi dengan kolom max dan sum untuk tiap dx_*_ind.\n",
        "    \"\"\"\n",
        "    # Agregasi max dan sum per patient_nbr\n",
        "    dx_aggregated = dx_events.groupby('patient_nbr').agg(\n",
        "        {f'dx_{dx}_ind': ['max', 'sum'] for dx in dx_list}\n",
        "    )\n",
        "\n",
        "    # Rename kolom jadi flat (contoh: dx_428_ind_max)\n",
        "    dx_aggregated.columns = [f'{col[0]}_{col[1]}' for col in dx_aggregated.columns]\n",
        "\n",
        "    # Reset index untuk membawa kembali patient_nbr sebagai kolom biasa\n",
        "    dx_aggregated = dx_aggregated.reset_index()\n",
        "\n",
        "    # Simpan ke CSV jika diminta\n",
        "    if output_csv:\n",
        "        dx_aggregated.to_csv(output_csv, index=False)\n",
        "\n",
        "    # Bersihkan memori (opsional, hanya jika dx_events tidak dipakai lagi)\n",
        "    # del dx_events\n",
        "\n",
        "    return dx_aggregated"
      ],
      "metadata": {
        "id": "BBO7TPju6Uhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx_aggregated_train = aggregate_dx_indicators(train_dx, dx_list, output_csv='dx_aggregated_train.csv')\n",
        "dx_aggregated_test = aggregate_dx_indicators(test_dx, dx_list, output_csv='dx_aggregated_test.csv')"
      ],
      "metadata": {
        "id": "kiitZmKH6Wqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_dx_aggregated(base_df: pd.DataFrame, dx_aggregated: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Menggabungkan dx_aggregated ke base_df berdasarkan 'patient_nbr' dan menghapus dx_aggregated dari memori.\n",
        "\n",
        "    Parameters:\n",
        "        base_df (pd.DataFrame): DataFrame utama (misalnya train_df, test_df, val_df).\n",
        "        dx_aggregated (pd.DataFrame): DataFrame hasil agregasi diagnosis yang akan digabungkan.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame hasil penggabungan.\n",
        "    \"\"\"\n",
        "    print(\"Sebelum merge:\", base_df.shape)\n",
        "    base_df = base_df.merge(dx_aggregated, on='patient_nbr', how='left')\n",
        "    print(\"Setelah merge:\", base_df.shape)\n",
        "    del dx_aggregated\n",
        "    return base_df\n"
      ],
      "metadata": {
        "id": "30lHru5Y6YmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = merge_dx_aggregated(train_df, dx_aggregated_train)\n",
        "test_df = merge_dx_aggregated(test_df, dx_aggregated_test)\n"
      ],
      "metadata": {
        "id": "PnP6xJ_R6aBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.11 Membuat Indikator berdasarkan word matching historis pasien"
      ],
      "metadata": {
        "id": "VQUA1WRj6bEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_history_indicators(df: pd.DataFrame, drop: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Menambahkan indikator untuk diagnosis spesifik (alcohol, obesity, malignant hypertension)\n",
        "    dan membuat versi historis (per patient_nbr) dari masing-masing indikator.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame dengan kolom diagnosis_1, diagnosis_2, diagnosis_3.\n",
        "        drop (list): List kolom yang akan diperluas untuk di-drop nantinya (akan di-extend).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan kolom tambahan history_ind untuk tiap diagnosis yang dimaksud.\n",
        "    \"\"\"\n",
        "\n",
        "    # Diagnosis spesifik (langsung pada encounter)\n",
        "    df['alcohol_ind'] = df[['diagnosis_1', 'diagnosis_2', 'diagnosis_3']].apply(\n",
        "        lambda row: int(any('ALCOHOL' in str(val) for val in row)), axis=1\n",
        "    )\n",
        "    df['obesity_ind'] = df[['diagnosis_1', 'diagnosis_2', 'diagnosis_3']].apply(\n",
        "        lambda row: int(any('OBESITY' in str(val) for val in row)), axis=1\n",
        "    )\n",
        "    df['mh_ind'] = df[['diagnosis_1', 'diagnosis_2', 'diagnosis_3']].apply(\n",
        "        lambda row: int(any('MALIGNANT HYPERTENSION' in str(val) for val in row)), axis=1\n",
        "    )\n",
        "\n",
        "    # Indikator historis berdasarkan patient_nbr\n",
        "    df['alcohol_history_ind'] = df.groupby('patient_nbr')['alcohol_ind'].transform('max')\n",
        "    df['obesity_history_ind'] = df.groupby('patient_nbr')['obesity_ind'].transform('max')\n",
        "    df['mh_history_ind'] = df.groupby('patient_nbr')['mh_ind'].transform('max')\n",
        "\n",
        "    # Tambahkan kolom sementara ke daftar drop\n",
        "    drop.extend(['alcohol_ind', 'obesity_ind', 'mh_ind'])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "EtDZ6mn76eeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop = []  # Pastikan list `drop` sudah didefinisikan\n",
        "train_df = create_history_indicators(train_df, drop)\n",
        "test_df = create_history_indicators(test_df, drop)\n"
      ],
      "metadata": {
        "id": "RwTNUdIc6iM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.12 Membuat fitur pada tingkat pasien (agregasi fitur pada level pasien)"
      ],
      "metadata": {
        "id": "airXRqHy6oCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_patient_level_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Menambahkan fitur-fitur agregat pada level pasien berdasarkan encounter yang dimiliki pasien tersebut.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame berisi kolom 'patient_nbr', 'encounter_id', dan kolom numerik lain.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan tambahan kolom fitur agregat per pasien.\n",
        "    \"\"\"\n",
        "    # Jumlah encounter unik per pasien\n",
        "    df['encounter_ct'] = df['patient_nbr'].map(df.groupby('patient_nbr')['encounter_id'].nunique())\n",
        "\n",
        "    # Agregasi jumlah total untuk berbagai variabel numerik\n",
        "    df['mb_time_in_hospital'] = df.groupby('patient_nbr')['time_in_hospital'].transform('sum')\n",
        "    df['mb_num_lab_procedures_ct'] = df.groupby('patient_nbr')['num_lab_procedures'].transform('sum')\n",
        "    df['mb_num_procedures_ct'] = df.groupby('patient_nbr')['num_procedures'].transform('sum')\n",
        "    df['mb_num_medications_ct'] = df.groupby('patient_nbr')['num_medications'].transform('sum')\n",
        "    df['mb_number_outpatient_ct'] = df.groupby('patient_nbr')['number_outpatient'].transform('sum')\n",
        "    df['mb_number_emergency_ct'] = df.groupby('patient_nbr')['number_emergency'].transform('sum')\n",
        "    df['mb_number_inpatient_ct'] = df.groupby('patient_nbr')['number_inpatient'].transform('sum')\n",
        "    df['mb_number_diagnoses_ct'] = df.groupby('patient_nbr')['number_diagnoses'].transform('sum')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kDvTapdK6yKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = add_patient_level_features(train_df)\n",
        "test_df = add_patient_level_features(test_df)"
      ],
      "metadata": {
        "id": "R6tiOdZJ6zl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.13 Membuat Indikator A1C"
      ],
      "metadata": {
        "id": "NvfOJn7G60wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_a1c_indicators(df: pd.DataFrame, drop: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Menambahkan indikator A1C (>7, >8, Normal) dan menghitung jumlah kemunculannya\n",
        "    per pasien untuk masing-masing kategori.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame yang memiliki kolom 'A1Cresult' dan 'patient_nbr'.\n",
        "        drop (list): List nama kolom yang akan dijatuhkan nantinya. Kolom indikator sementara akan ditambahkan ke list ini.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame dengan kolom indikator A1C dan agregasi per pasien.\n",
        "    \"\"\"\n",
        "\n",
        "    # Buat kolom indikator berdasarkan nilai A1Cresult\n",
        "    df['A1c gt 7 ind'] = (df['A1Cresult'] == '>7').astype(int)\n",
        "    df['A1c gt 8 ind'] = (df['A1Cresult'] == '>8').astype(int)\n",
        "    df['A1c Norm ind'] = (df['A1Cresult'] == 'Norm').astype(int)\n",
        "\n",
        "    # Hitung agregasi jumlah per pasien untuk masing-masing indikator\n",
        "    df['mb A1c gt 7 ct'] = df.groupby('patient_nbr')['A1c gt 7 ind'].transform('sum')\n",
        "    df['mb A1c gt 8 ct'] = df.groupby('patient_nbr')['A1c gt 8 ind'].transform('sum')\n",
        "    df['mb A1c Norm ct'] = df.groupby('patient_nbr')['A1c Norm ind'].transform('sum')\n",
        "\n",
        "    # Masukkan kolom sementara ke dalam list drop\n",
        "    drop.extend(['A1c gt 7 ind', 'A1c gt 8 ind', 'A1c Norm ind'])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "gtCBK9Ji64JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop = []  # pastikan variabel drop sudah didefinisikan\n",
        "train_df = add_a1c_indicators(train_df, drop)\n",
        "test_df = add_a1c_indicators(test_df, drop)\n"
      ],
      "metadata": {
        "id": "16Sri8JN65gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.14 Melakukan seleski obat"
      ],
      "metadata": {
        "id": "65FJQ67C67Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#get frequency on all these.\n",
        "drugs=['metformin'\n",
        ",'repaglinide'\n",
        ",'nateglinide'\n",
        ",'chlorpropamide'\n",
        ",'glimepiride'\n",
        ",'acetohexamide'\n",
        ",'glipizide'\n",
        ",'glyburide'\n",
        ",'tolbutamide'\n",
        ",'pioglitazone'\n",
        ",'rosiglitazone'\n",
        ",'acarbose'\n",
        ",'miglitol'\n",
        ",'troglitazone'\n",
        ",'tolazamide'\n",
        ",'examide'\n",
        ",'citoglipton'\n",
        ",'insulin'\n",
        ",'glyburide-metformin'\n",
        ",'glipizide-metformin'\n",
        ",'glimepiride-pioglitazone'\n",
        ",'metformin-rosiglitazone'\n",
        ",'metformin-pioglitazone']"
      ],
      "metadata": {
        "id": "TZ8qZgj77Bd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an empty list to store results\n",
        "records = []\n",
        "\n",
        "# Compute frequency counts for each feature in `drugs`\n",
        "for feature in drugs:\n",
        "    value_counts = diabetes[feature].value_counts(dropna=False).reset_index()\n",
        "    value_counts.columns = ['observation', 'count']\n",
        "    value_counts['feature'] = feature  # Add feature name\n",
        "\n",
        "    # Append results\n",
        "    records.append(value_counts)\n",
        "\n",
        "# Combine all frequency tables into a single DataFrame\n",
        "drug_freq = pd.concat(records, ignore_index=True)[['feature', 'observation', 'count']]\n",
        "drug_freq.to_csv('drugs.csv',index=False)"
      ],
      "metadata": {
        "id": "8UuyKY4k7C9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop.extend([ 'citoglipton'\n",
        ", 'examide'\n",
        ", 'acetohexamide'\n",
        ", 'glimepiride-pioglitazone'\n",
        ", 'metformin-pioglitazone'\n",
        ", 'metformin-rosiglitazone'\n",
        ", 'troglitazone'\n",
        ", 'glipizide-metformin'\n",
        ", 'tolbutamide'\n",
        ", 'miglitol'\n",
        ", 'tolazamide'\n",
        ", 'chlorpropamide'\n",
        ", 'acarbose'\n",
        ", 'nateglinide'\n",
        ", 'glyburide-metformin'\n",
        ", 'repaglinide' ])"
      ],
      "metadata": {
        "id": "1tC2r_Uk7EQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_drug_drop_columns(drop: list) -> list:\n",
        "    \"\"\"\n",
        "    Menambahkan nama-nama kolom obat yang tidak digunakan lagi ke dalam list drop.\n",
        "\n",
        "    Parameters:\n",
        "        drop (list): List kolom yang akan di-drop nantinya.\n",
        "\n",
        "    Returns:\n",
        "        list: List drop yang sudah diperluas dengan kolom obat.\n",
        "    \"\"\"\n",
        "    drop.extend\n"
      ],
      "metadata": {
        "id": "yMIq_16P7F7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop = extend_drug_drop_columns(drop)"
      ],
      "metadata": {
        "id": "xFbxxWA47HK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.15 Finalisasi Dataset"
      ],
      "metadata": {
        "id": "Ad7Go0rP7LBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def finalize_dataframe(df: pd.DataFrame, drop_cols: list, csv_filename: str = None, pkl_filename: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Men-drop kolom tertentu, menambahkan kolom dummy, menyimpan ke CSV & Pickle jika diminta.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame input (train/test/val).\n",
        "        drop_cols (list): Daftar nama kolom yang ingin di-drop.\n",
        "        csv_filename (str, optional): Nama file untuk menyimpan CSV. Default None (tidak disimpan).\n",
        "        pkl_filename (str, optional): Nama file untuk menyimpan Pickle. Default None (tidak disimpan).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame yang telah dimodifikasi.\n",
        "    \"\"\"\n",
        "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
        "    df['dummy'] = 1\n",
        "\n",
        "    if csv_filename:\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    if pkl_filename:\n",
        "        df.to_pickle(pkl_filename)\n",
        "\n",
        "    print(f\"✅ DataFrame shape: {df.shape}\")\n",
        "    print(df.info())\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "7ekFjIb_7KQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop = [\n",
        "    'race',\n",
        "    'weight',\n",
        "    'medical_specialty',\n",
        "    'max_glu_serum',\n",
        "    'payer_code',\n",
        "    'admission_type_id',\n",
        "    'discharge_disposition_id',\n",
        "    'admission_source_id',\n",
        "    'admission_grp_1_ind',\n",
        "    'admission_grp_2_ind',\n",
        "    'discharge_grp_1_ind',\n",
        "    'discharge_grp_2_ind',\n",
        "    'admission_type_ind',\n",
        "    'alcohol_ind',\n",
        "    'obesity_ind',\n",
        "    'mh_ind',\n",
        "    'A1c gt 7 ind',\n",
        "    'A1c gt 8 ind',\n",
        "    'A1c Norm ind',\n",
        "    'citoglipton',\n",
        "    'examide',\n",
        "    'acetohexamide',\n",
        "    'glimepiride-pioglitazone',\n",
        "    'metformin-pioglitazone',\n",
        "    'metformin-rosiglitazone',\n",
        "    'troglitazone',\n",
        "    'glipizide-metformin',\n",
        "    'tolbutamide',\n",
        "    'miglitol',\n",
        "    'tolazamide',\n",
        "    'chlorpropamide',\n",
        "    'acarbose',\n",
        "    'nateglinide',\n",
        "    'glyburide-metformin',\n",
        "    'repaglinide',\n",
        "    'mb_under10_ind',\n",
        "    'mb_over80_ind',\n",
        "    'expiration_ind'\n",
        "]\n"
      ],
      "metadata": {
        "id": "5jPzKKuj7TD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "drop.extend([ 'diag_1', 'diag_2', 'diag_3', 'diagnosis_tuple_freq'\n",
        ", 'diag_1_freq', 'diag_2_freq', 'diag_3_freq'\n",
        ", 'diagnosis_1', 'diagnosis_2', 'diagnosis_3'\n",
        ",  'diag_1_sort' ,'diag_2_sort' ,'diag_3_sort'\n",
        ", 'diagnosis_1_sort', 'diagnosis_2_sort', 'diagnosis_3_sort'\n",
        ", 'diag_1_sort_freq', 'diag_2_sort_freq', 'diag_3_sort_freq'])"
      ],
      "metadata": {
        "id": "HS5jz5Ve7UZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = finalize_dataframe(train_df, drop, csv_filename='train_df.csv', pkl_filename='train_df.pkl')\n",
        "test_df = finalize_dataframe(test_df, drop, csv_filename='test_df.csv', pkl_filename='test_df.pkl')\n"
      ],
      "metadata": {
        "id": "PxeIthmc7Vi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modelling"
      ],
      "metadata": {
        "id": "RLH9kHfc7YLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Import Data setelah Feature Engineering"
      ],
      "metadata": {
        "id": "evqMLIvjeZ5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = 'p24'\n",
        "train= pd.read_pickle('/content/drive/MyDrive/Skripsi/Dataset/diabetes+130-us+hospitals+for+years+1999-2008/V8 (COBA COBA)/train_df(5).pkl')\n",
        "test= pd.read_pickle('/content/drive/MyDrive/Skripsi/Dataset/diabetes+130-us+hospitals+for+years+1999-2008/V8 (COBA COBA)/test_df(5).pkl')\n",
        "for col in train.select_dtypes(include='int64'):\n",
        "    train[col] = train[col].astype('int32')\n",
        "\n",
        "for col in test.select_dtypes(include='int64'):\n",
        "    test[col] = test[col].astype('int32')\n",
        "\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n"
      ],
      "metadata": {
        "id": "Wd2DlO1ueeEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Split X dan Y"
      ],
      "metadata": {
        "id": "4wti_Sc3elBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=train.drop(['readmitted_ind','readmitted','encounter_id','patient_nbr','dummy'], axis=1) # Buang kedua kolom target\n",
        "y_train=train['readmitted_ind']"
      ],
      "metadata": {
        "id": "pj5Us0cwen2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test=test.drop(['readmitted_ind','readmitted','encounter_id','patient_nbr','dummy'], axis=1) # Buang kedua kolom target\n",
        "y_test=test['readmitted_ind']"
      ],
      "metadata": {
        "id": "4901ojA5epIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output ukuran dataset untuk verifikasi\n",
        "print(f\"Bentuk Data Training Awal: {X_train.shape}\")\n",
        "print(f\"Bentuk Data Testing: {X_test.shape}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "l1bU40fUeraF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Menyatakan cat_features"
      ],
      "metadata": {
        "id": "hmBI673afPHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features = [\n",
        "    'A1Cresult',\n",
        "    'MP_DM_payer_ind',\n",
        "    'admission_source',\n",
        "    'admission_type',\n",
        "    'age',\n",
        "    'alcohol_history_ind',\n",
        "    'change',\n",
        "    'diag_1_428_ind',\n",
        "    'diag_1_491_ind',\n",
        "    'diag_1_493_ind',\n",
        "    'diag_1_driver_ind',\n",
        "    'diag_2_403_ind',\n",
        "    'diag_2_491_ind',\n",
        "    'diag_2_585_ind',\n",
        "    'diag_2_707_ind',\n",
        "    'diag_2_driver_ind',\n",
        "    'diag_3_403_ind',\n",
        "    'diag_3_585_ind',\n",
        "    'diag_3_driver_ind',\n",
        "    'diabetesMed',\n",
        "    'diagnosis_tuple',\n",
        "    'discharge_disposition',\n",
        "    'dx_282_ind_max', 'dx_536_ind_max', 'dx_581_ind_max',\n",
        "    'dx_403_ind_max', 'dx_585_ind_max', 'dx_583_ind_max', 'dx_491_ind_max',\n",
        "    'dx_537_ind_max', 'dx_443_ind_max', 'dx_277_ind_max', 'dx_730_ind_max',\n",
        "    'dx_428_ind_max', 'dx_404_ind_max',\n",
        "    'gender',\n",
        "    'glimepiride',\n",
        "    'glipizide',\n",
        "    'glyburide',\n",
        "    'high_propensity_tuple_ind',\n",
        "    'insulin',\n",
        "    'metformin',\n",
        "    'mh_history_ind',\n",
        "    'obesity_history_ind',\n",
        "    'pioglitazone',\n",
        "    'rosiglitazone',\n",
        "    'diagnosis_tuple'\n",
        "]\n",
        "\n",
        "print(f\"Jumlah Fitur Kategorik: {len(object_features)}\")"
      ],
      "metadata": {
        "id": "CNBxzselfbl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Bersihkan Missing values yang ada pada data"
      ],
      "metadata": {
        "id": "KwGzjn3XfoDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bersihkan NaN di semua subset data (train, val, test)\n",
        "for col in cat_features:\n",
        "    if col in X_train.columns:\n",
        "        X_train[col] = X_train[col].fillna('_MISSING_').astype(str)\n",
        "        X_test[col]  = X_test[col].fillna('_MISSING_').astype(str)\n"
      ],
      "metadata": {
        "id": "Q5mKgRjWf2gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Membuat Fungsi Evaluasi Metriks"
      ],
      "metadata": {
        "id": "4ZABufpOf_Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification_model(model, namestring, token,\n",
        "                                  X_train, y_train,\n",
        "                                  X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates a classification model across train, validation, and test sets,\n",
        "    saves metrics and predictions to a pickle file.\n",
        "    \"\"\"\n",
        "\n",
        "    # === 1. Validasi input ===\n",
        "    if any(v is None for v in [X_train, X_test, y_train, y_test]):\n",
        "        raise ValueError(\"🚨 Salah satu dari X_train, X_test, y_train, y_test bernilai None.\")\n",
        "\n",
        "    y_train = y_train.values if hasattr(y_train, \"values\") else y_train\n",
        "    y_test  = y_test.values if hasattr(y_test, \"values\") else y_test\n",
        "\n",
        "    # === 2. Prediksi label (0/1) ===\n",
        "    print(\"Making predictions...\")\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred  = model.predict(X_test)\n",
        "\n",
        "    # === 3. Cek probabilitas atau confidence score ===\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        print('✅ model.predict_proba() digunakan')\n",
        "        y_train_pred_pct = model.predict_proba(X_train)[:, 1]\n",
        "        y_test_pred_pct  = model.predict_proba(X_test)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        print('✅ model.decision_function() digunakan')\n",
        "        y_train_pred_pct = model.decision_function(X_train)\n",
        "        y_test_pred_pct  = model.decision_function(X_test)\n",
        "    else:\n",
        "        raise AttributeError(\"🚨 Model tidak memiliki predict_proba() maupun decision_function(). Tidak bisa hitung AUC/ROC.\")\n",
        "\n",
        "    # === 4. Hitung metrik ===\n",
        "    acc_train = accuracy_score(y_train, y_train_pred)\n",
        "    acc_test  = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_pct)\n",
        "    fpr_test, tpr_test, thresholds = roc_curve(y_test, y_test_pred_pct)\n",
        "\n",
        "    auc_train = auc(fpr_train, tpr_train)\n",
        "    auc_test  = auc(fpr_test, tpr_test)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    sensitivity = recall_score(y_test, y_test_pred)\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    precision   = precision_score(y_test, y_test_pred)\n",
        "    f1score     = f1_score(y_test, y_test_pred)\n",
        "\n",
        "    # === 5. Simpan hasil ===\n",
        "    print(\"Saving results...\")\n",
        "    result_data = {\n",
        "        'model_version': f\"{token}_{namestring}\",\n",
        "        'accuracy_train': acc_train,\n",
        "        'accuracy_test': acc_test,\n",
        "        'auc_train': auc_train,\n",
        "        'auc_test': auc_test,\n",
        "        'sensitivity_test': sensitivity,\n",
        "        'specificity_test': specificity,\n",
        "        'precision_test': precision,\n",
        "        'f1_score_test': f1score,\n",
        "        'confusion_matrix_components': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp},\n",
        "        'roc_curve_data': {'fpr': fpr_test, 'tpr': tpr_test, 'thresholds': thresholds},\n",
        "        'y_test': y_test,\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'y_test_pred_pct': y_test_pred_pct,\n",
        "        'display_labels': model.classes_ if hasattr(model, \"classes_\") else [0, 1]\n",
        "    }\n",
        "\n",
        "    filename = f\"model_results_{token}_{namestring}.pkl\"\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(result_data, f)\n",
        "    print(f\"Results saved as {filename}\")\n",
        "\n",
        "    # === 6. Tampilkan Ringkasan ===\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"MODEL PERFORMANCE SUMMARY: {namestring}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Metric':<18} | {'Train':<10} | {'Test':<10}\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"{'Accuracy':<18} | {acc_train:<10.4f} | {acc_test:<10.4f}\")\n",
        "    print(f\"{'AUC':<18} | {auc_train:<10.4f} | {auc_test:<10.4f}\")\n",
        "    print(\"\\n--- Test Set Detail Metrics ---\")\n",
        "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "    print(f\"Specificity        : {specificity:.4f}\")\n",
        "    print(f\"Precision          : {precision:.4f}\")\n",
        "    print(f\"F1 Score           : {f1score:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # === 7. Plot Confusion Matrix ===\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=result_data['display_labels'])\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(f\"Confusion Matrix - Test Set ({namestring})\")\n",
        "    plt.show()\n",
        "\n",
        "    return result_data\n"
      ],
      "metadata": {
        "id": "vtgxoX3LgG1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Membangun Base Model"
      ],
      "metadata": {
        "id": "brq0JCt4f4In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRULY BASE MODEL - Parameter Default Semua\n",
        "base_model = CatBoostClassifier(\n",
        "    random_seed=42,           # Untuk reproducibility\n",
        "    cat_features=cat_features # Wajib untuk categorical features\n",
        "    # TIDAK ADA PARAMETER LAIN!\n",
        ")"
      ],
      "metadata": {
        "id": "Kfl4VrpFf6gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.fit(\n",
        "    X_train, y_train,verbose=100\n",
        ")"
      ],
      "metadata": {
        "id": "AaHF4tNif9Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_classification_model(\n",
        "    model=base_model,\n",
        "    namestring='Based_Model_CatBoost',\n",
        "    token=token,\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    X_test=X_test, y_test=y_test\n",
        ")"
      ],
      "metadata": {
        "id": "6DML4gDAgLnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7 Hyperparameter Tuning Randomized Search dengan Cross Validation"
      ],
      "metadata": {
        "id": "DIZF49MugQAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 0: Asumsi Data & Fitur Kategorik Sudah Siap\n",
        "# ==============================================================================\n",
        "# Pastikan variabel berikut sudah tersedia:\n",
        "# - X_train, y_train: Data training\n",
        "# - cat_features: List index atau nama kolom kategorik di X_train\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 1: Definisikan Ruang Pencarian Hyperparameter untuk CatBoost\n",
        "# ==============================================================================\n",
        "param_dist_cat = {\n",
        "    'iterations': [300, 500, 1000],                    # Kurangi pilihan\n",
        "    'learning_rate': [0.008, 0.01, 0.012],     # Fokus di sekitar 0.01\n",
        "    'depth': [10, 11, 12],                       # Sekitar base model\n",
        "    'l2_leaf_reg': [4, 6, 8],                  # Sekitar base model\n",
        "    'bagging_temperature': [0.8, 1.0, 1.2],\n",
        "    'border_count': [128, 254],\n",
        "    'random_strength': [1, 5, 10]\n",
        "}\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 2: Siapkan Model Dasar dan RandomizedSearchCV\n",
        "# ==============================================================================\n",
        "# Hitung scale_pos_weight untuk mengatasi imbalance\n",
        "scale_pos_weight_value = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
        "\n",
        "cat_base = CatBoostClassifier(\n",
        "    random_state=42,\n",
        "    scale_pos_weight=scale_pos_weight_value,\n",
        "    verbose=0  # Hindari output training yang panjang\n",
        ")\n",
        "\n",
        "# Kurangi n_iter dan CV\n",
        "random_search_cat = RandomizedSearchCV(\n",
        "    estimator=cat_base,\n",
        "    param_distributions=param_dist_cat,\n",
        "    n_iter=15,           # 🔥 Drastis kurangi dari 50 ke 15\n",
        "    cv=5,                # 🔥 Kurangi dari 5-fold ke 3-fold\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    random_state=43\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 3: Jalankan Pencarian Hyperparameter\n",
        "# ==============================================================================\n",
        "print(\"🚀 Memulai proses Hyperparameter Tuning untuk CatBoost...\")\n",
        "\n",
        "random_search_cat.fit(\n",
        "    X_train, y_train,\n",
        "    cat_features=cat_features  # Penting: beritahu fitur kategorik\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 4: Simpan dan Tampilkan Hasil\n",
        "# ==============================================================================\n",
        "token = \"v8_basemodel_improve\"\n",
        "filename = f\"{token}_catboost_tuned_search.pkl\"\n",
        "\n",
        "with open(filename, \"wb\") as file:\n",
        "    pickle.dump(random_search_cat, file)\n",
        "\n",
        "print(f\"\\n📦 Objek hasil pencarian disimpan sebagai: {filename}\")\n",
        "print(\"🔥 Hyperparameter Terbaik untuk CatBoost:\")\n",
        "print(best_params_cat := random_search_cat.best_params_)  # Python 3.8+: walrus operator\n",
        "print(f\"📈 Skor AUC terbaik dari Cross-Validation: {random_search_cat.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "Gj9QaOcygV5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 Run Model dengan paramater terbaik dan evaluasi model"
      ],
      "metadata": {
        "id": "fkwqxk2WggYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Memanggil best param dari pkl\n",
        "import pickle\n",
        "\n",
        "# Muat kembali objek hasil pencarian\n",
        "with open('/content/drive/MyDrive/Skripsi/Dataset/diabetes+130-us+hospitals+for+years+1999-2008/CatBoost V7 (BaseModel Improve)/v7_basemodel_improve_catboost_tuned_search.pkl', 'rb') as file:\n",
        "    loaded_search_object = pickle.load(file)\n",
        "\n",
        "# Ambil kembali parameter terbaik dari objek tersebut\n",
        "best_params_cat = loaded_search_object.best_params_\n",
        "\n",
        "print(\"Parameter terbaik yang dimuat dari file:\")\n",
        "print(best_params_cat)\n",
        "\n",
        "scale_pos_weight_value = np.sum(y_train == 0) / np.sum(y_train == 1)"
      ],
      "metadata": {
        "id": "Ol0JwEaDhrl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pastikan fungsi evaluate_classification_model sudah di-import atau didefinisikan di atas\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 1: Asumsi Variabel dari Langkah Sebelumnya Sudah Tersedia\n",
        "# ==============================================================================\n",
        "# Pastikan variabel-variabel ini sudah ada dari proses tuning sebelumnya:\n",
        "# - best_params_cat: Dictionary berisi hyperparameter terbaik.\n",
        "# - scale_pos_weight_value: Nilai scale_pos_weight yang sudah dihitung.\n",
        "# - X_train, y_train, X_val, y_val, X_test, y_test: Data split yang sudah ada.\n",
        "# - cat_features: List nama kolom kategorik.\n",
        "# - token: Variabel token untuk penamaan file.\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 2: Buat dan Latih Model Final dengan Parameter Terbaik\n",
        "# ==============================================================================\n",
        "print(\"🚀 Melatih model CatBoost final dengan hyperparameter terbaik...\")\n",
        "\n",
        "# Buat instance model final dengan menggabungkan parameter terbaik dan parameter tetap\n",
        "final_cat_model = CatBoostClassifier(\n",
        "    **best_params_cat,\n",
        "    scale_pos_weight=scale_pos_weight_value,\n",
        "    random_state=42,\n",
        "    verbose=100  # Tampilkan progres setiap 100 iterasi\n",
        ")\n",
        "\n",
        "# Latih model pada SELURUH data training, gunakan data validasi untuk early stopping\n",
        "# %%time # Anda bisa uncomment ini di Jupyter untuk mengukur waktu\n",
        "final_cat_model.fit(\n",
        "    X_train, y_train,\n",
        "    cat_features=cat_features,\n",
        "    early_stopping_rounds=50,\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 3: Simpan Model Final yang Sudah Dilatih\n",
        "# ==============================================================================\n",
        "filename = f\"{token}_final_catboost_model.pkl\"\n",
        "with open(filename, \"wb\") as file:\n",
        "    pickle.dump(final_cat_model, file)\n",
        "print(f\"\\n📦 Model final yang sudah dilatih disimpan sebagai: {filename}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Tahap 4: Evaluasi Komprehensif pada Model Final\n",
        "# ==============================================================================\n",
        "print(\"\\n📊 Mengevaluasi kinerja model final...\")\n",
        "evaluate_classification_model(\n",
        "    model=final_cat_model,\n",
        "    namestring='CatBoost_Tuned_Final',\n",
        "    token=token,\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    X_test=X_test, y_test=y_test\n",
        ")"
      ],
      "metadata": {
        "id": "NmsWypfhget3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9 Feature Importance"
      ],
      "metadata": {
        "id": "6zuwJJOxiAhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Asumsikan variabel berikut sudah tersedia:\n",
        "# - final_cat_model: Model CatBoost yang sudah dilatih.\n",
        "# - X_train: DataFrame pandas yang digunakan untuk melatih (untuk mengambil nama kolom).\n",
        "\n",
        "# a. Dapatkan skor dan buat DataFrame\n",
        "feature_importances = final_cat_model.get_feature_importance()\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# b. Urutkan dari yang paling penting\n",
        "sorted_importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# c. Tampilkan 20 fitur paling penting dalam bentuk tabel\n",
        "print(\"🔥 Top 40 Fitur Paling Penting (Bentuk DataFrame):\")\n",
        "print(sorted_importance_df.head(40))\n",
        "\n",
        "\n",
        "# d. Buat visualisasi dalam bentuk grafik batang\n",
        "print(\"\\n📊 Membuat Grafik Feature Importance...\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(\n",
        "    x='Importance',\n",
        "    y='Feature',\n",
        "    data=sorted_importance_df.head(20) # Ambil 20 teratas untuk visualisasi\n",
        ")\n",
        "plt.title('Top 20 Fitur Paling Penting - Model CatBoost', fontsize=16)\n",
        "plt.xlabel('Skor Importance', fontsize=12)\n",
        "plt.ylabel('Nama Fitur', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mcwgPdvDiHz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from matplotlib.patches import Rectangle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class FeatureImportanceAnalyzer:\n",
        "    \"\"\"\n",
        "    Kelas untuk analisis feature importance yang komprehensif dengan berbagai visualisasi\n",
        "    dan insight mendalam untuk model machine learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, X_train, feature_names=None):\n",
        "        \"\"\"\n",
        "        Inisialisasi analyzer\n",
        "\n",
        "        Parameters:\n",
        "        - model: Model yang sudah dilatih (CatBoost, XGBoost, RandomForest, dll)\n",
        "        - X_train: DataFrame training data\n",
        "        - feature_names: List nama fitur (opsional, akan diambil dari X_train.columns)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.X_train = X_train\n",
        "        self.feature_names = feature_names if feature_names else X_train.columns.tolist()\n",
        "        self.importance_df = None\n",
        "\n",
        "    def get_feature_importance(self):\n",
        "        \"\"\"Extract feature importance dari model\"\"\"\n",
        "        try:\n",
        "            # Untuk CatBoost\n",
        "            if hasattr(self.model, 'get_feature_importance'):\n",
        "                importances = self.model.get_feature_importance()\n",
        "            # Untuk sklearn models (RandomForest, XGBoost, dll)\n",
        "            elif hasattr(self.model, 'feature_importances_'):\n",
        "                importances = self.model.feature_importances_\n",
        "            else:\n",
        "                raise AttributeError(\"Model tidak memiliki feature importance\")\n",
        "\n",
        "            self.importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'Importance': importances,\n",
        "                'Importance_Normalized': importances / importances.sum() * 100\n",
        "            }).sort_values('Importance', ascending=False)\n",
        "\n",
        "            # Tambah ranking\n",
        "            self.importance_df['Rank'] = range(1, len(self.importance_df) + 1)\n",
        "\n",
        "            return self.importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saat mengambil feature importance: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_comprehensive_report(self, top_n=20, save_plots=False, plot_dir=\"./\"):\n",
        "        \"\"\"\n",
        "        Membuat laporan komprehensif feature importance\n",
        "\n",
        "        Parameters:\n",
        "        - top_n: Jumlah top fitur yang ditampilkan\n",
        "        - save_plots: Apakah menyimpan plot ke file\n",
        "        - plot_dir: Direktori untuk menyimpan plot\n",
        "        \"\"\"\n",
        "\n",
        "        if self.importance_df is None:\n",
        "            self.get_feature_importance()\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"🚀 COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # 1. Statistik Dasar\n",
        "        self._print_basic_statistics(top_n)\n",
        "\n",
        "        # 2. Analisis Distribusi\n",
        "        self._print_distribution_analysis()\n",
        "\n",
        "        # 3. Visualisasi\n",
        "        self._create_visualizations(top_n, save_plots, plot_dir)\n",
        "\n",
        "        # 4. Rekomendasi\n",
        "        self._print_recommendations()\n",
        "\n",
        "    def _print_basic_statistics(self, top_n):\n",
        "        \"\"\"Cetak statistik dasar feature importance\"\"\"\n",
        "        print(f\"\\n📊 STATISTIK DASAR FEATURE IMPORTANCE\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        total_features = len(self.importance_df)\n",
        "        top_features = self.importance_df.head(top_n)\n",
        "\n",
        "        print(f\"Total Fitur: {total_features}\")\n",
        "        print(f\"Skor Importance Tertinggi: {self.importance_df.iloc[0]['Importance']:.4f}\")\n",
        "        print(f\"Skor Importance Terendah: {self.importance_df.iloc[-1]['Importance']:.4f}\")\n",
        "        print(f\"Rata-rata Skor: {self.importance_df['Importance'].mean():.4f}\")\n",
        "        print(f\"Median Skor: {self.importance_df['Importance'].median():.4f}\")\n",
        "        print(f\"Standard Deviasi: {self.importance_df['Importance'].std():.4f}\")\n",
        "\n",
        "        # Kontribusi kumulatif top features\n",
        "        cumulative_importance = top_features['Importance_Normalized'].sum()\n",
        "        print(f\"\\n🎯 Top {top_n} fitur berkontribusi: {cumulative_importance:.2f}% dari total importance\")\n",
        "\n",
        "        # Tampilkan top features dalam tabel yang rapi\n",
        "        print(f\"\\n🔥 TOP {top_n} FITUR PALING PENTING:\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        display_df = top_features[['Rank', 'Feature', 'Importance', 'Importance_Normalized']].copy()\n",
        "        display_df['Importance'] = display_df['Importance'].round(4)\n",
        "        display_df['Importance_Normalized'] = display_df['Importance_Normalized'].round(2)\n",
        "        display_df.columns = ['Rank', 'Feature Name', 'Raw Score', '% Contribution']\n",
        "\n",
        "        print(display_df.to_string(index=False, max_colwidth=25))\n",
        "\n",
        "    def _print_distribution_analysis(self):\n",
        "        \"\"\"Analisis distribusi feature importance\"\"\"\n",
        "        print(f\"\\n📈 ANALISIS DISTRIBUSI IMPORTANCE\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Kategorisasi fitur berdasarkan importance\n",
        "        high_threshold = self.importance_df['Importance'].quantile(0.8)\n",
        "        medium_threshold = self.importance_df['Importance'].quantile(0.5)\n",
        "\n",
        "        high_importance = self.importance_df[self.importance_df['Importance'] >= high_threshold]\n",
        "        medium_importance = self.importance_df[\n",
        "            (self.importance_df['Importance'] >= medium_threshold) &\n",
        "            (self.importance_df['Importance'] < high_threshold)\n",
        "        ]\n",
        "        low_importance = self.importance_df[self.importance_df['Importance'] < medium_threshold]\n",
        "\n",
        "        print(f\"🔴 Fitur Importance Tinggi (≥ P80): {len(high_importance)} fitur\")\n",
        "        print(f\"🟡 Fitur Importance Sedang (P50-P80): {len(medium_importance)} fitur\")\n",
        "        print(f\"🔵 Fitur Importance Rendah (< P50): {len(low_importance)} fitur\")\n",
        "\n",
        "        # Rasio importance fitur teratas vs terendah\n",
        "        top_importance = self.importance_df.iloc[0]['Importance']\n",
        "        bottom_importance = self.importance_df.iloc[-1]['Importance']\n",
        "        ratio = top_importance / bottom_importance if bottom_importance > 0 else float('inf')\n",
        "\n",
        "        print(f\"\\n📏 Rasio fitur terpenting vs terlemah: {ratio:.2f}x\")\n",
        "\n",
        "        # Cek apakah ada fitur dengan importance 0\n",
        "        zero_importance = self.importance_df[self.importance_df['Importance'] == 0]\n",
        "        if len(zero_importance) > 0:\n",
        "            print(f\"⚠️  Ditemukan {len(zero_importance)} fitur dengan importance = 0\")\n",
        "\n",
        "    def _create_visualizations(self, top_n, save_plots, plot_dir):\n",
        "        \"\"\"Buat berbagai visualisasi feature importance\"\"\"\n",
        "        print(f\"\\n🎨 MEMBUAT VISUALISASI...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Setup untuk multiple subplots\n",
        "        fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "        # 1. Horizontal Bar Plot (Top Features)\n",
        "        ax1 = plt.subplot(2, 3, 1)\n",
        "        top_data = self.importance_df.head(top_n)\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(top_data)))\n",
        "\n",
        "        bars = ax1.barh(range(len(top_data)), top_data['Importance'], color=colors)\n",
        "        ax1.set_yticks(range(len(top_data)))\n",
        "        ax1.set_yticklabels(top_data['Feature'], fontsize=10)\n",
        "        ax1.set_xlabel('Feature Importance Score')\n",
        "        ax1.set_title(f'Top {top_n} Most Important Features', fontsize=14, fontweight='bold')\n",
        "        ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "        # Tambahkan nilai pada bar\n",
        "        for i, (bar, value) in enumerate(zip(bars, top_data['Importance'])):\n",
        "            ax1.text(value + max(top_data['Importance']) * 0.01, bar.get_y() + bar.get_height()/2,\n",
        "                    f'{value:.3f}', va='center', fontsize=9)\n",
        "\n",
        "        # 2. Pie Chart (Top 10 + Others)\n",
        "        ax2 = plt.subplot(2, 3, 2)\n",
        "        top_10 = self.importance_df.head(10)\n",
        "        others_sum = self.importance_df.iloc[10:]['Importance_Normalized'].sum()\n",
        "\n",
        "        pie_data = list(top_10['Importance_Normalized']) + [others_sum]\n",
        "        pie_labels = list(top_10['Feature']) + ['Others']\n",
        "\n",
        "        wedges, texts, autotexts = ax2.pie(pie_data, labels=pie_labels, autopct='%1.1f%%',\n",
        "                                          startangle=90, textprops={'fontsize': 9})\n",
        "        ax2.set_title('Feature Importance Distribution\\n(Top 10 + Others)', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # 3. Cumulative Importance Plot\n",
        "        ax3 = plt.subplot(2, 3, 3)\n",
        "        cumulative = self.importance_df['Importance_Normalized'].cumsum()\n",
        "        ax3.plot(range(1, len(cumulative) + 1), cumulative, 'b-', linewidth=2, marker='o', markersize=3)\n",
        "        ax3.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
        "        ax3.axhline(y=95, color='orange', linestyle='--', alpha=0.7, label='95% threshold')\n",
        "        ax3.set_xlabel('Number of Features')\n",
        "        ax3.set_ylabel('Cumulative Importance (%)')\n",
        "        ax3.set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        ax3.legend()\n",
        "\n",
        "        # Tandai berapa fitur untuk mencapai 80% dan 95%\n",
        "        features_80 = (cumulative >= 80).idxmax() + 1\n",
        "        features_95 = (cumulative >= 95).idxmax() + 1\n",
        "        ax3.axvline(x=features_80, color='r', linestyle=':', alpha=0.7)\n",
        "        ax3.axvline(x=features_95, color='orange', linestyle=':', alpha=0.7)\n",
        "        ax3.text(features_80, 40, f'{features_80} features\\nfor 80%', ha='center',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"red\", alpha=0.2))\n",
        "\n",
        "        # 4. Box Plot untuk distribusi importance\n",
        "        ax4 = plt.subplot(2, 3, 4)\n",
        "        ax4.boxplot(self.importance_df['Importance'], vert=True, patch_artist=True,\n",
        "                   boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
        "                   medianprops=dict(color='red', linewidth=2))\n",
        "        ax4.set_ylabel('Feature Importance')\n",
        "        ax4.set_title('Distribution of Feature Importance', fontsize=14, fontweight='bold')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        # Tambahkan statistik pada plot\n",
        "        stats_text = f'Mean: {self.importance_df[\"Importance\"].mean():.4f}\\n'\n",
        "        stats_text += f'Median: {self.importance_df[\"Importance\"].median():.4f}\\n'\n",
        "        stats_text += f'Std: {self.importance_df[\"Importance\"].std():.4f}'\n",
        "        ax4.text(0.7, 0.95, stats_text, transform=ax4.transAxes, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.3))\n",
        "\n",
        "        # 5. Histogram distribusi importance\n",
        "        ax5 = plt.subplot(2, 3, 5)\n",
        "        ax5.hist(self.importance_df['Importance'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax5.axvline(self.importance_df['Importance'].mean(), color='red', linestyle='--',\n",
        "                   linewidth=2, label=f'Mean: {self.importance_df[\"Importance\"].mean():.4f}')\n",
        "        ax5.axvline(self.importance_df['Importance'].median(), color='green', linestyle='--',\n",
        "                   linewidth=2, label=f'Median: {self.importance_df[\"Importance\"].median():.4f}')\n",
        "        ax5.set_xlabel('Feature Importance')\n",
        "        ax5.set_ylabel('Frequency')\n",
        "        ax5.set_title('Histogram of Feature Importance', fontsize=14, fontweight='bold')\n",
        "        ax5.legend()\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. Feature Importance vs Rank (Log scale)\n",
        "        ax6 = plt.subplot(2, 3, 6)\n",
        "        ax6.scatter(self.importance_df['Rank'], self.importance_df['Importance'],\n",
        "                   alpha=0.6, s=50, c=self.importance_df['Importance'], cmap='viridis')\n",
        "        ax6.set_xlabel('Feature Rank')\n",
        "        ax6.set_ylabel('Feature Importance (Log Scale)')\n",
        "        ax6.set_yscale('log')\n",
        "        ax6.set_title('Feature Importance vs Rank (Log Scale)', fontsize=14, fontweight='bold')\n",
        "        ax6.grid(True, alpha=0.3)\n",
        "\n",
        "        # Tambahkan colorbar\n",
        "        cbar = plt.colorbar(ax6.collections[0], ax=ax6)\n",
        "        cbar.set_label('Importance Score')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_plots:\n",
        "            plt.savefig(f'{plot_dir}/feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
        "            print(f\"📁 Plot disimpan ke: {plot_dir}/feature_importance_analysis.png\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def _print_recommendations(self):\n",
        "        \"\"\"Berikan rekomendasi berdasarkan analisis\"\"\"\n",
        "        print(f\"\\n💡 REKOMENDASI & INSIGHTS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Hitung berapa fitur untuk mencapai threshold tertentu\n",
        "        cumulative = self.importance_df['Importance_Normalized'].cumsum()\n",
        "        features_50 = (cumulative >= 50).idxmax() + 1\n",
        "        features_80 = (cumulative >= 80).idxmax() + 1\n",
        "        features_95 = (cumulative >= 95).idxmax() + 1\n",
        "\n",
        "        total_features = len(self.importance_df)\n",
        "\n",
        "        print(f\"📌 Untuk mencapai 50% dari total importance: {features_50} fitur\")\n",
        "        print(f\"📌 Untuk mencapai 80% dari total importance: {features_80} fitur\")\n",
        "        print(f\"📌 Untuk mencapai 95% dari total importance: {features_95} fitur\")\n",
        "\n",
        "        # Rekomendasi feature selection\n",
        "        print(f\"\\n🎯 REKOMENDASI FEATURE SELECTION:\")\n",
        "\n",
        "        if features_80 / total_features < 0.3:\n",
        "            print(f\"✅ Model cukup efisien: hanya {features_80} dari {total_features} fitur ({features_80/total_features*100:.1f}%) untuk 80% importance\")\n",
        "            print(f\"   → Pertimbangkan menggunakan top {features_80} fitur untuk model yang lebih sederhana\")\n",
        "        else:\n",
        "            print(f\"⚠️  Model menggunakan banyak fitur: {features_80} dari {total_features} fitur untuk 80% importance\")\n",
        "            print(f\"   → Pertimbangkan feature engineering atau regularization lebih kuat\")\n",
        "\n",
        "        # Cek dominasi fitur teratas\n",
        "        top_5_contribution = self.importance_df.head(5)['Importance_Normalized'].sum()\n",
        "        if top_5_contribution > 60:\n",
        "            print(f\"⚠️  Top 5 fitur mendominasi ({top_5_contribution:.1f}% importance)\")\n",
        "            print(f\"   → Periksa apakah ada overfitting atau data leakage\")\n",
        "\n",
        "        # Rekomendasi berdasarkan distribusi\n",
        "        zero_importance_count = len(self.importance_df[self.importance_df['Importance'] == 0])\n",
        "        if zero_importance_count > 0:\n",
        "            print(f\"🗑️  Ditemukan {zero_importance_count} fitur dengan importance = 0\")\n",
        "            print(f\"   → Pertimbangkan menghapus fitur-fitur ini\")\n",
        "\n",
        "        low_importance_count = len(self.importance_df[\n",
        "            self.importance_df['Importance'] < self.importance_df['Importance'].quantile(0.1)\n",
        "        ])\n",
        "        if low_importance_count > total_features * 0.2:\n",
        "            print(f\"📉 {low_importance_count} fitur memiliki importance sangat rendah (< P10)\")\n",
        "            print(f\"   → Pertimbangkan feature selection untuk mengurangi noise\")\n",
        "\n",
        "        print(f\"\\n🔍 INSIGHTS TAMBAHAN:\")\n",
        "        print(f\"• Rasio importance tertinggi/terendah: {self.importance_df.iloc[0]['Importance']/self.importance_df.iloc[-1]['Importance']:.1f}x\")\n",
        "        print(f\"• Coefficient of variation: {self.importance_df['Importance'].std()/self.importance_df['Importance'].mean():.3f}\")\n",
        "\n",
        "        if self.importance_df['Importance'].std()/self.importance_df['Importance'].mean() > 1:\n",
        "            print(\"  → Distribusi importance cukup varied (bagus untuk interpretability)\")\n",
        "        else:\n",
        "            print(\"  → Distribusi importance relatif seragam (banyak fitur berkontribusi similar)\")\n",
        "\n",
        "    def get_feature_groups_analysis(self, feature_groups=None):\n",
        "        \"\"\"\n",
        "        Analisis importance berdasarkan grup fitur\n",
        "\n",
        "        Parameters:\n",
        "        - feature_groups: Dict dengan key=nama grup, value=list fitur dalam grup\n",
        "        \"\"\"\n",
        "        if feature_groups is None:\n",
        "            print(\"ℹ️  Tidak ada definisi grup fitur. Melewati analisis grup.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n🏷️  ANALISIS BERDASARKAN GRUP FITUR\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        group_importance = {}\n",
        "        for group_name, features in feature_groups.items():\n",
        "            # Filter fitur yang ada di importance_df\n",
        "            valid_features = [f for f in features if f in self.importance_df['Feature'].values]\n",
        "            if valid_features:\n",
        "                group_data = self.importance_df[self.importance_df['Feature'].isin(valid_features)]\n",
        "                group_importance[group_name] = {\n",
        "                    'total_importance': group_data['Importance'].sum(),\n",
        "                    'avg_importance': group_data['Importance'].mean(),\n",
        "                    'feature_count': len(valid_features),\n",
        "                    'top_feature': group_data.loc[group_data['Importance'].idxmax(), 'Feature'],\n",
        "                    'top_importance': group_data['Importance'].max()\n",
        "                }\n",
        "\n",
        "        # Buat DataFrame untuk grup\n",
        "        group_df = pd.DataFrame(group_importance).T\n",
        "        group_df = group_df.sort_values('total_importance', ascending=False)\n",
        "\n",
        "        print(\"📊 Importance per Grup Fitur:\")\n",
        "        print(group_df.round(4))\n",
        "\n",
        "        # Visualisasi grup\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Subplot 1: Total importance per grup\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.bar(group_df.index, group_df['total_importance'], color='skyblue', alpha=0.8)\n",
        "        plt.title('Total Importance per Feature Group')\n",
        "        plt.xlabel('Feature Group')\n",
        "        plt.ylabel('Total Importance')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Subplot 2: Average importance per grup\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.bar(group_df.index, group_df['avg_importance'], color='lightcoral', alpha=0.8)\n",
        "        plt.title('Average Importance per Feature Group')\n",
        "        plt.xlabel('Feature Group')\n",
        "        plt.ylabel('Average Importance')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return group_df\n",
        "\n",
        "# Fungsi wrapper untuk penggunaan yang mudah\n",
        "def analyze_feature_importance(model, X_train, top_n=20, save_plots=False,\n",
        "                             feature_groups=None, plot_dir=\"./\"):\n",
        "    \"\"\"\n",
        "    Fungsi wrapper untuk analisis feature importance yang komprehensif\n",
        "\n",
        "    Parameters:\n",
        "    - model: Model yang sudah dilatih\n",
        "    - X_train: DataFrame training data\n",
        "    - top_n: Jumlah top fitur yang ditampilkan\n",
        "    - save_plots: Apakah menyimpan plot\n",
        "    - feature_groups: Dict grup fitur (opsional)\n",
        "    - plot_dir: Direktori untuk menyimpan plot\n",
        "\n",
        "    Returns:\n",
        "    - analyzer: Object FeatureImportanceAnalyzer\n",
        "    - importance_df: DataFrame dengan feature importance\n",
        "    \"\"\"\n",
        "\n",
        "    analyzer = FeatureImportanceAnalyzer(model, X_train)\n",
        "    importance_df = analyzer.get_feature_importance()\n",
        "\n",
        "    if importance_df is not None:\n",
        "        analyzer.create_comprehensive_report(top_n, save_plots, plot_dir)\n",
        "\n",
        "        if feature_groups:\n",
        "            analyzer.get_feature_groups_analysis(feature_groups)\n",
        "\n",
        "    return analyzer, importance_df\n",
        "\n",
        "# Contoh penggunaan:\n",
        "\"\"\"\n",
        "# Untuk model CatBoost\n",
        "analyzer, importance_df = analyze_feature_importance(\n",
        "    model=final_cat_model,\n",
        "    X_train=X_train,\n",
        "    top_n=25,\n",
        "    save_plots=True,\n",
        "    plot_dir=\"./plots/\"\n",
        ")\n",
        "\n",
        "# Dengan definisi grup fitur (opsional)\n",
        "feature_groups = {\n",
        "    'Demographics': ['age', 'gender', 'education'],\n",
        "    'Financial': ['income', 'debt', 'credit_score'],\n",
        "    'Behavioral': ['purchase_frequency', 'website_visits']\n",
        "}\n",
        "\n",
        "analyzer, importance_df = analyze_feature_importance(\n",
        "    model=final_cat_model,\n",
        "    X_train=X_train,\n",
        "    top_n=20,\n",
        "    feature_groups=feature_groups\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wRCy35twin6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer, importance_df = analyze_feature_importance(\n",
        "    model=final_cat_model,\n",
        "    X_train=X_train,\n",
        "    top_n=50,\n",
        "    save_plots=True\n",
        ")"
      ],
      "metadata": {
        "id": "o0ueBB7_iq_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def create_simple_pie_chart(model, X_train, top_n=5, figsize=(10, 10),\n",
        "                           save_plot=False, plot_dir=\"./\", filename=\"feature_pie.png\",\n",
        "                           title=\"Feature Importance Analysis\"):\n",
        "    \"\"\"\n",
        "    Membuat pie chart feature importance yang sangat sederhana dan clean\n",
        "\n",
        "    Parameters:\n",
        "    - model: Model yang sudah dilatih\n",
        "    - X_train: DataFrame training data\n",
        "    - top_n: Jumlah top fitur (default: 5)\n",
        "    - figsize: Ukuran figure (default: (10, 10))\n",
        "    - save_plot: Simpan ke file (default: False)\n",
        "    - plot_dir: Direktori penyimpanan\n",
        "    - filename: Nama file\n",
        "    - title: Judul utama plot (default: \"Feature Importance Analysis\")\n",
        "\n",
        "    Returns:\n",
        "    - importance_df: DataFrame feature importance\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract feature importance\n",
        "    try:\n",
        "        feature_names = X_train.columns.tolist()\n",
        "\n",
        "        if hasattr(model, 'get_feature_importance'):\n",
        "            importances = model.get_feature_importance()\n",
        "        elif hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "        else:\n",
        "            raise AttributeError(\"Model tidak memiliki feature importance\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importances,\n",
        "            'Importance_Normalized': importances / importances.sum() * 100\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Data untuk pie chart\n",
        "    top_features = importance_df.head(top_n)\n",
        "    others_sum = importance_df.iloc[top_n:]['Importance_Normalized'].sum() if len(importance_df) > top_n else 0\n",
        "\n",
        "    pie_data = list(top_features['Importance_Normalized'])\n",
        "    pie_labels = list(top_features['Feature'])\n",
        "\n",
        "    if others_sum > 0:\n",
        "        pie_data.append(others_sum)\n",
        "        pie_labels.append('Others')\n",
        "\n",
        "    # Pie chart dengan style seperti contoh gambar\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Warna yang kontras seperti contoh\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "\n",
        "    wedges, texts, autotexts = plt.pie(\n",
        "        pie_data,\n",
        "        labels=pie_labels,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=90,\n",
        "        colors=colors[:len(pie_data)],\n",
        "        pctdistance=0.85,  # Jarak percentage dari center\n",
        "        labeldistance=1.1,  # Jarak label dari center\n",
        "        textprops={'fontsize': 14}  # Font size diperbesar untuk semua text\n",
        "    )\n",
        "\n",
        "    # Pengaturan font untuk labels (nama fitur) - posisi di luar - FONT HITAM\n",
        "    for text in texts:\n",
        "        text.set_fontsize(16)  # Diperbesar dari 12 ke 16\n",
        "        text.set_fontweight('bold')  # Dibuat bold\n",
        "        text.set_color('black')\n",
        "\n",
        "    # Pengaturan font untuk percentages - di dalam pie - FONT HITAM\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('black')  # Diubah dari white ke black\n",
        "        autotext.set_fontsize(14)    # Diperbesar dari 10 ke 14\n",
        "        autotext.set_fontweight('bold')\n",
        "\n",
        "    # Judul utama di atas plot - posisi lebih tinggi\n",
        "    plt.suptitle(title, fontsize=20, fontweight='bold', y=0.98)\n",
        "\n",
        "    # Sub-judul untuk detail - spacing lebih besar\n",
        "    plt.title(f'Top {top_n} Most Important Features + Others',\n",
        "              fontsize=14, fontweight='normal', pad=40)\n",
        "    plt.axis('equal')\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.92])  # Beri ruang untuk judul di atas\n",
        "\n",
        "    if save_plot:\n",
        "        plt.savefig(f'{plot_dir}/{filename}', dpi=300, bbox_inches='tight')\n",
        "        print(f\"📁 Plot saved: {plot_dir}/{filename}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print ringkasan\n",
        "    total_features = len(importance_df)\n",
        "    top_contrib = sum(pie_data[:-1]) if others_sum > 0 else sum(pie_data)\n",
        "\n",
        "    print(f\"\\n📊 Top {top_n} dari {total_features} fitur ({top_contrib:.1f}%)\")\n",
        "    for i, row in importance_df.head(top_n).iterrows():\n",
        "        print(f\"{i+1}. {row['Feature']}: {row['Importance_Normalized']:.1f}%\")\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Contoh penggunaan:\n",
        "print(\"Contoh penggunaan:\")\n",
        "print(\"1. Dengan judul default:\")\n",
        "print(\"importance_df = create_simple_pie_chart(model=final_cat_model, X_train=X_train, top_n=5)\")\n",
        "print(\"\\n2. Dengan judul custom:\")\n",
        "print(\"importance_df = create_simple_pie_chart(model=final_cat_model, X_train=X_train, top_n=5, title='Healthcare Model - Key Features')\")"
      ],
      "metadata": {
        "id": "BGiNH-Bhivtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dengan judul custom\n",
        "importance_df = create_simple_pie_chart(\n",
        "    model=final_cat_model,\n",
        "    X_train=X_train,\n",
        "    top_n=5,\n",
        "    title=\"Model CatBoost - Feature Importance\"\n",
        ")"
      ],
      "metadata": {
        "id": "n26E1QpKiyxu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}